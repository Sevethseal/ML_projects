# üè¶ Loan Approval Prediction

## üéØ Objective

Predict **Loan_Status** (approved vs. not approved) from applicant and loan attributes using a clean preprocessing pipeline, feature engineering, and a multi‚Äëmodel comparison.

## üìä Dataset Overview

### Key Fields

- **Identifier**: `Loan_ID` (dropped for modeling)
- **Categorical Features**: `Gender`, `Married`, `Dependents`, `Education`, `Self_Employed`, `Property_Area`, `Loan_Status` (target)
- **Numerical Features**: `ApplicantIncome`, `CoapplicantIncome`, `LoanAmount`, `Loan_Amount_Term`, `Credit_History`

## üîÑ Workflow Summary

### 1. üîç Exploration & Missingness

- Assessed per‚Äëcolumn missing percentages and visualized missingness to understand data quality

### 2. üìÇ Type Separation

- Split columns into **numerical** vs. **non‚Äënumerical** to apply targeted cleaning strategies

### 3. üõ†Ô∏è Missing Value Handling

- **Numerical columns** with missing values ‚Üí filled with **median**
- **Non‚Äënumerical columns** with missing values ‚Üí filled with **mode** (with safe fallback for all‚ÄëNaN cases)

### 4. üóëÔ∏è Identifier Removal

- Dropped `Loan_ID` (no predictive value; prevents high‚Äëcardinality dummy explosion)

### 5. üî¢ Encoding

- **One‚Äëhot encoded** categorical predictors: `Gender`, `Married`, `Dependents`, `Education`, `Self_Employed`, `Property_Area` (reference category dropped to avoid redundancy)
- **Label‚Äëencoded** the target `Loan_Status` ‚Üí binary (Y‚Üí1, N‚Üí0)

### 6. ‚öôÔ∏è Feature Engineering

- Created **TotalIncome** (Applicant + Coapplicant)
- Created **LoanToIncome** (LoanAmount / TotalIncome) to capture burden
- Created **EMI** (LoanAmount / Loan_Amount_Term) as a simple repayment proxy
- Created **Log_TotalIncome** to reduce skew and stabilize variance

### 7. ü§ñ Modeling & Evaluation

- Compared multiple algorithms using **Stratified 5‚ÄëFold Cross‚ÄëValidation**
- Tracked **Accuracy** as the primary metric; also monitored **ROC‚ÄëAUC, F1, Precision, Recall** for a fuller view

## üß™ Models Compared

| Model Type            | Algorithm                           | Purpose                           |
| --------------------- | ----------------------------------- | --------------------------------- |
| **Baseline**          | Logistic Regression                 | Explainable, regularized baseline |
| **Tree Ensembles**    | Random Forest, HistGradientBoosting | Robust ensemble methods           |
| **Gradient Boosting** | LightGBM, XGBoost, CatBoost         | Tabular data SOTA performance     |

## üìà Selection Criterion

- Chose the **highest cross‚Äëvalidated Accuracy** as the primary selector
- Considered trade‚Äëoffs using **ROC‚ÄëAUC** and **Recall/Precision** depending on business costs (false approvals vs. false rejections)

## üîÑ End‚Äëto‚ÄëEnd Workflow (Reproducible)

```mermaid
flowchart TD
    A[1. Load Raw Data] --> B[2. Inspect Missingness]
    B --> C[3. Separate Numerical vs Non-Numerical]
    C --> D[4. Impute: Numerical‚ÜíMedian, Non-Numerical‚ÜíMode]
    D --> E[5. Drop Loan_ID]
    E --> F[6. One-Hot Encode Predictors, Label-Encode Target]
    F --> G[7. Feature Engineering]
    G --> H[8. Split Features/Target]
    H --> I[9. Cross-Validate Models]
    I --> J[10. Rank by Accuracy]
    J --> K[11. Select Best Model]
```

### Detailed Steps:

1. **Load raw data**
2. **Inspect and quantify missingness**
3. **Separate numerical vs. non‚Äënumerical columns**
4. **Impute**: numerical‚Üímedian; non‚Äënumerical‚Üímode
5. **Drop `Loan_ID`**
6. **One‚Äëhot encode predictors**; **label‚Äëencode `Loan_Status`**
7. **Engineer features**: TotalIncome, LoanToIncome, EMI, Log_TotalIncome
8. **Split features/target**
9. **Cross‚Äëvalidate models** (Logistic Regression, Random Forest, HistGradientBoosting, LightGBM, XGBoost, CatBoost) with stratified folds
10. **Rank by Accuracy**; review AUC/F1/Precision/Recall
11. **Select best model** for further tuning and deployment

## üìä Expected Results

- **Tree‚Äëbased gradient boosting** typically performs best on this dataset
- **Logistic Regression** provides an interpretable baseline reference

## üöÄ Next Steps (Optional)

### Model Optimization

- [ ] **Hyperparameter tuning** for the top model (learning rate, depth/leaves, regularization)
- [ ] **Probability calibration** and decision‚Äëthreshold optimization based on business cost

### Model Interpretability

- [ ] **Feature importance analysis**
- [ ] **SHAP analysis**
- [ ] **Fairness checks** across key demographic groups

### Deployment

- [ ] **Model packaging** for inference
- [ ] **Simple monitoring** (drift detection, performance tracking)

## üõ†Ô∏è Technologies Used

![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Pandas](https://img.shields.io/badge/Pandas-150458?style=for-the-badge&logo=pandas&logoColor=white)
![NumPy](https://img.shields.io/badge/NumPy-013243?style=for-the-badge&logo=numpy&logoColor=white)
![Scikit-learn](https://img.shields.io/badge/Scikit--learn-F7931E?style=for-the-badge&logo=scikit-learn&logoColor=white)
![LightGBM](https://img.shields.io/badge/LightGBM-02569B?style=for-the-badge&logo=lightgbm&logoColor=white)
![XGBoost](https://img.shields.io/badge/XGBoost-FF6600?style=for-the-badge&logo=xgboost&logoColor=white)

### Core Libraries:

- **Data Manipulation**: Pandas, NumPy
- **Machine Learning**: scikit‚Äëlearn
- **Gradient Boosting**: LightGBM, XGBoost, CatBoost
- **Visualization**: Matplotlib, Seaborn

‚≠ê If you found this project helpful, please give it a star!
