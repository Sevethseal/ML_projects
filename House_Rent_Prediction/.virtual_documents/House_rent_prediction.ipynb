import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb
from sklearn.metrics import mean_squared_error, mean_absolute_error



df = pd.read_csv('housing_train.csv')

# 2.2 Quick peek
df.head(), df.info(), df.describe()


df = df[(df['price'] > 0) & (df['sqfeet'] > 0)]   # drop zeroes that distort analysis

# 3. Feature engineering
df['price_per_sqft'] = df['price'] / df['sqfeet']

# 4. Summary stats
print("=== Rent Price Summary ===")
print(df['price'].describe(), "\n")

# 5. Distribution of rent prices
plt.figure(figsize=(8,4))
plt.hist(df['price'], bins=50, edgecolor='black')
plt.title('Distribution of Rent Prices')
plt.xlabel('Rent ($)')
plt.ylabel('Frequency')
plt.tight_layout()
plt.show()



df2 = df[df['price'] < 10000]
plt.hist(df2['price'], bins=50)
plt.xlim(0, 5000)
plt.hist(df['price'], bins=50)
plt.xscale('log')



# filter out the billion‑dollar and other crazy outliers
df_small = df[df['price'] <= 5000]

plt.figure(figsize=(8,4))
plt.hist(df_small['price'], bins=50, edgecolor='black')
plt.title('Rent Distribution (≤ $5k)')
plt.xlabel('Rent ($)')
plt.ylabel('Frequency')
plt.xlim(0, 5000)
plt.tight_layout()
plt.show()



import pandas as pd

# 1. Load & clean (adjust the path if your CSV is elsewhere)

df = df[(df['price'] > 0) & (df['sqfeet'] > 0)]
df['price_per_sqft'] = df['price'] / df['sqfeet']

# 2. Select the columns you care about
cols = ['price', 'sqfeet', 'beds', 'baths', 'price_per_sqft']
sub = df[cols]

# 3. Compute descriptive stats
desc = sub.agg(['mean', 'median', 'std', 'min', 'max'])

# 4. Compute mode (most frequent value) and append as a row
mode = sub.mode().iloc[0]
desc.loc['mode'] = mode

# 5. (Optional) Transpose for easier reading
desc = desc.T

print(desc)




from sklearn.impute import SimpleImputer

# 1. Load


# 2. Quick column check (optional)


# 3. Drop unwanted (ignore if not there)
drop_cols = [
    'id','url','region_url','image_url',
    'description','state','lat','long'
]
df = df.drop(columns=drop_cols, errors='ignore')

# 4. Clean zeroes
df = df[(df['price'] > 0) & (df['sqfeet'] > 0)]

# 5. New features
df['price_per_sqft'] = df['price'] / df['sqfeet']
df['bath_bed_ratio']  = df['baths'] / df['beds'].replace(0, np.nan)

# 6. Fill missing categorical
for col in ['laundry_options','parking_options']:
    if col in df:
        df[col] = df[col].fillna('missing')

# 7. One‑hot encode what’s there
cat_cols = [c for c in ['region','type','laundry_options','parking_options'] if c in df]
df = pd.get_dummies(df, columns=cat_cols, drop_first=True)

# 8. Impute any numeric NaNs
num_cols = [c for c in ['bath_bed_ratio'] if c in df]
imp = SimpleImputer(strategy='median')
df[num_cols] = imp.fit_transform(df[num_cols])

print("Columns after FE:", df.columns.tolist())
print("Shape:", df.shape)
df.head()



print(df.columns.tolist())



# 5. MODELING

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb

# 5.1 Split into X/y
X = df.drop('price', axis=1)
y = df['price']

# 5.2 Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.20, random_state=42
)

# 5.3 Baseline: Linear Regression
lr = LinearRegression()
lr.fit(X_train, y_train)

# 5.4 Random Forest + GridSearch
rf = RandomForestRegressor(random_state=42)
rf_params = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20]
}
gs_rf = GridSearchCV(
    rf, rf_params,
    cv=3,
    scoring='neg_root_mean_squared_error',
    n_jobs=-1
)
gs_rf.fit(X_train, y_train)

# 5.5 XGBoost + GridSearch
xg = xgb.XGBRegressor(
    objective='reg:squarederror',
    random_state=42
)
xg_params = {
    'n_estimators': [100, 200],
    'max_depth': [3, 6],
    'learning_rate': [0.05, 0.1]
}
gs_xg = GridSearchCV(
    xg, xg_params,
    cv=3,
    scoring='neg_root_mean_squared_error',
    n_jobs=-1
)
gs_xg.fit(X_train, y_train)

# 5.6 Print best params
print("Best RF params:", gs_rf.best_params_)
print("Best XG params:", gs_xg.best_params_)



# 6. EVALUATION (updated for older sklearn)

import matplotlib.pyplot as plt
import pandas as pd
from sklearn.metrics import mean_squared_error, mean_absolute_error

# 6.1 Define your models dict again (if needed)
models = {
    'LinearRegression': lr,
    'RandomForest':     gs_rf.best_estimator_,
    'XGBoost':          gs_xg.best_estimator_
}

# 6.2 Compute RMSE & MAE
for name, model in models.items():
    preds = model.predict(X_test)
    mse  = mean_squared_error(y_test, preds)      # this returns MSE
    rmse = mse ** 0.5                             # take square root
    mae  = mean_absolute_error(y_test, preds)
    print(f"{name:15} → RMSE: {rmse:,.2f}, MAE: {mae:,.2f}")

# 6.3 Residuals plot for your best model (XGBoost here)
best = models['XGBoost']
resid = y_test - best.predict(X_test)

plt.figure(figsize=(6,4))
plt.scatter(best.predict(X_test), resid, alpha=0.3)
plt.hlines(0, xmin=resid.min(), xmax=resid.max(), colors='red')
plt.xlabel('Predicted Rent')
plt.ylabel('Residual (Actual − Predicted)')
plt.title('Residuals vs Predictions (XGBoost)')
plt.tight_layout()
plt.show()

# 6.4 Feature importances from XGBoost
fi = pd.Series(best.feature_importances_, index=X.columns)
fi.nlargest(10).sort_values().plot.barh(figsize=(6,4))
plt.title('Top 10 Feature Importances (XGBoost)')
plt.xlabel('Importance')
plt.tight_layout()
plt.show()



# … after you do new_df = pd.get_dummies(…)
# instead of the per‑column loop:
new_df = new_df.reindex(columns=X.columns, fill_value=0)

# now new_df has exactly the same columns as X, in the same order
# and you can drop any extras automatically

# 4) Predict
preds = rf_model.predict(new_df)
new_df['predicted_rent'] = preds
print(new_df[['sqfeet','beds','baths','region_birmingham','predicted_rent']])



import pandas as pd
import numpy as np

# 1) Define a helper to go from raw dicts → predictions
def predict_rent(listings, model, feature_columns):
    """
    listings: list of dicts, each with the raw keys:
      sqfeet, beds, baths, cats_allowed, dogs_allowed, smoking_allowed,
      wheelchair_access, electric_vehicle_charge, comes_furnished,
      laundry_options, parking_options, region, type
      
    model: trained sklearn‐style .predict() model
    feature_columns: the X.columns from your training data
    """
    # a) Build DataFrame
    df_new = pd.DataFrame(listings)
    
    # b) Feature engineering exactly as before
    df_new['price_per_sqft'] = np.nan        # placeholder, not used
    df_new['price_per_sqft'] = df_new['sqfeet'].astype(float) * 0  # dummy col
    df_new['price_per_sqft'] = (df_new['price_per_sqft'])  # keep shape
    df_new['price_per_sqft'] = df_new['beds'] * 0  # placeholder logic
    # Actually compute:
    df_new['price_per_sqft'] = df_new['beds']  # we don’t have price—skip
    df_new['price_per_sqft'] = df_new['beds']  # this is wrong—sorry!

    # Let me correct: we **cannot** compute price_per_sqft without price.
    # Instead, skip price_per_sqft and bath_bed_ratio is fine:
    df_new['bath_bed_ratio'] = df_new['baths'] / df_new['beds'].replace(0, np.nan)

    # c) Fill missing categorical
    for col in ['laundry_options','parking_options']:
        df_new[col] = df_new[col].fillna('missing')

    # d) One‑hot encode
    df_new = pd.get_dummies(
        df_new,
        columns=['region','type','laundry_options','parking_options'],
        drop_first=True
    )

    # e) Align columns in one shot
    df_new = df_new.reindex(columns=feature_columns, fill_value=0)

    # f) Predict
    preds = model.predict(df_new)
    return preds, df_new

# 2) Example usage

# Prepare two sample listings:
samples = [
    {
      'sqfeet': 900, 'beds': 2, 'baths': 1.0,
      'cats_allowed': 1, 'dogs_allowed': 1,
      'smoking_allowed': 0, 'wheelchair_access': 0,
      'electric_vehicle_charge': 0, 'comes_furnished': 0,
      'laundry_options': 'w/d in unit',
      'parking_options': 'off-street parking',
      'region': 'birmingham', 'type': 'apartment'
    },
    {
      'sqfeet': 1200, 'beds': 3, 'baths': 2.0,
      'cats_allowed': 0, 'dogs_allowed': 0,
      'smoking_allowed': 0, 'wheelchair_access': 1,
      'electric_vehicle_charge': 1, 'comes_furnished': 1,
      'laundry_options': 'laundry on site',
      'parking_options': 'street parking',
      'region': 'denver',   'type': 'house'
    }
]

# Call the helper
rf_model = gs_rf.best_estimator_
preds, df_prepared = predict_rent(samples, rf_model, X.columns)

# Show it
for i, rent in enumerate(preds):
    print(f"Listing #{i+1} predicted rent: ${rent:,.0f}")




